<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://compping.github.io/blog</id>
    <title>My Site Blog</title>
    <updated>2023-09-14T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://compping.github.io/blog"/>
    <subtitle>My Site Blog</subtitle>
    <icon>https://compping.github.io/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[Recommdenr System RL Introduction]]></title>
        <id>https://compping.github.io/blog/recsys-rl-intro</id>
        <link href="https://compping.github.io/blog/recsys-rl-intro"/>
        <updated>2023-09-14T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[추천 시스템에 대해 더 자세히 이해하고 싶다면 아래 튜토리얼 코드를 참고하기 바랍니다.]]></summary>
        <content type="html"><![CDATA[<p>추천 시스템에 대해 더 자세히 이해하고 싶다면 아래 튜토리얼 코드를 참고하기 바랍니다.</p><blockquote><p>Tutorial Code: <a href="https://github.com/DevSlem/recommender-system-rl-tutorial" target="_blank" rel="noopener noreferrer">DevSlem/recommender-system-rl-tutorial (Github)</a></p></blockquote><p><strong>추천 시스템</strong> (recommender system)은 유저의 선호도 (preference)에 맞는 아이템을 제공하는 시스템입니다. 이는 유저-아이템 상호작용 히스토리를 고려해 이루어지는데, 추천 시스템이 유저에게 아이템을 제공하면 유저는 이에 대해 <strong>피드백</strong> (스킵, 클릭, 구매 등)을 제공합니다. 유튜브, 넷플릭스 등 수 많은 어플리케이션에서 이러한 추천 시스템을 도입하고 있습니다.</p><p><img loading="lazy" src="/assets/images/recsys-d473d6e4f27bdd6eda6df5bef2d65719.png" width="668" height="374" class="img_ev3q"></p><p>추천 시스템은 머신 러닝 (machine learning)을 통해 구축할 수 있습니다. 지도학습 (supervised learning)과 같은 기존 방법들은 대체적으로 <strong>유저와 추천 모델 사이의 상호작용을 무시</strong>해 불만족스러운 결과를 내놓습니다. 일반적으로, 추천 시스템은 인터렉티브한 프로세스로 <strong>연속적인 의사 결정 문제</strong> (sequential decision making problem)입니다. 따라서 <strong>강화학습</strong> (reinforcement learning)을 사용하여 최적화할 수 있습니다. 아래 그림은 지도학습과 강화학습 기반 방법 사이의 성능 비교 테이블입니다.</p><p><img loading="lazy" src="/assets/images/recsys-performance-table-a04cf8567c3d7ca13e8da3c6403217af.png" width="1572" height="392" class="img_ev3q"></p><p>이 포스트에서는 강화학습으로 추천 시스템을 구축하는 것에 대한 간단한 소개를 하려고 합니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="recommdenr-system-process">Recommdenr System Process<a href="#recommdenr-system-process" class="hash-link" aria-label="Recommdenr System Process에 대한 직접 링크" title="Recommdenr System Process에 대한 직접 링크">​</a></h2><p>추천 시스템은 크게 두 과정으로 나뉩니다.</p><ol><li>candidate generation</li><li>ranking and recommendation</li></ol><p><img loading="lazy" src="/assets/images/recsys-process-3cdbcb35c9416067a3291e9592051b64.png" width="844" height="540" class="img_ev3q"></p><p>candidate generation은 수많은 아이템 중 일부분을 추출하는 과정입니다. 너무 많은 아이템을 모델에 입력하는 것은 비효율적이기 때문에 사전에 걸러내는 작업입니다. 이 때 후보 아이템 set을 <strong>document</strong>라고 부릅니다. ranking and recommendation은 document 아이템 중에서 실제 유저에게 추천할 아이템을 선택하는 과정입니다. 여기에 머신 러닝과 같은 기법이 사용됩니다. document로부터 선택된 아이템 set을 <strong>slate</strong>라고 부릅니다. </p><p><a href="https://github.com/google-research/recsim" target="_blank" rel="noopener noreferrer">Google RecSim</a>은 유저와의 연속적인 상호작용을 지원하는 추천 시스템에 대한 시뮬레이션 environment로, youtube 추천 알고리즘을 위해 개발되었습니다. 아래는 RecSim 아키텍쳐를 나타내는 그림으로 지금까지 설명한 내용을 한번에 보여주고 있습니다.</p><p><img loading="lazy" src="/assets/images/recsim-5dfba8e67865e1ed9962ac6af1123598.png" width="1032" height="686" class="img_ev3q"></p><p>자, 이제 toy 문제를 보고 왜 강화학습이 유용한지 알아봅시다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="problem-recommend-items-based-on-sweetness">Problem: Recommend Items based on Sweetness<a href="#problem-recommend-items-based-on-sweetness" class="hash-link" aria-label="Problem: Recommend Items based on Sweetness에 대한 직접 링크" title="Problem: Recommend Items based on Sweetness에 대한 직접 링크">​</a></h2><p>초콜릿과 케일 (채소)이 여러 개 있습니다. 우리는 초콜릿과 케일 중 어떤 것을 추천해야 유저가 만족할까를 고민하고 있습니다. 초콜릿은 단 맛이고, 케일은 쓴 맛이기 때문에 초콜릿과 케일을 달콤함 (sweetness)로 나타낼 수 있습니다. 여기서는 단순함을 위해 달콤함만 고려합시다. 유저들은 대체적으로 쓴 음식보다는 달콤한 음식을 선호할 것입니다. 우리가 생각해볼 수 있는 방법은 달콤한 음식만 추천하는 것입니다. 그러나 달콤한 음식만 추천하다보면 유저들은 점점 만족스러워하지 않을 것입니다. 왜냐하면 자신의 건강 역시 생각하기 떄문이죠. 따라서 유저들은 달콤한 음식보다는 건강에 좋은 달콤하지 않은 음식을 점점 더 선호하게 될 가능성이 있습니다. 그러나 대체적으로 달콤한 음식을 선호하는 유저들이 지속적으로 쓴 음식만 추천 받는다면 역시 불만족스럽겠죠. 이러한 요소들을 종합하면 우리의 가설은 다음과 같습니다: <strong>유저들은 대체적으로 달콤한 음식을 선호하지만, 시간이 지나면서 점점 달콤한 음식의 선호도가 내려가기 때문에 중간 중간 달콥하지 않은 음식도 추천 받길 원한다.</strong></p><p><img loading="lazy" src="/assets/images/user-choice-model-0a4b9ee6900e861beb837f4e049db2ab.png" width="1300" height="686" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="reinforcement-learning">Reinforcement Learning<a href="#reinforcement-learning" class="hash-link" aria-label="Reinforcement Learning에 대한 직접 링크" title="Reinforcement Learning에 대한 직접 링크">​</a></h2><p>위 문제를 강화학습으로 학습하기 위해 추천 시스템의 요소들을 잘 정의해야합니다. 추천 시스템의 궁극적인 목적은 유저의 engagement를 maximize하는 것입니다.</p><p>Objective: Maximize user's engagement.</p><p>여기서 engagment란 단어가 다소 모호할 수 있습니다. engagement란 추천된 아이템에 대한 상호작용이나 유저의 행동으로, <strong>유저의 흥미나 관심을 얼마나 효과적으로 끌고 있는지를 나타내는 측정값</strong>입니다. 예를 들면, 추천된 동영상을 시청한 시간 정도 입니다.</p><p>이제 다음을 정의해봅시다:</p><ul><li>Observation: sweetness of 20 items</li><li>Action: recommends 1 item</li><li>Reward: represents the engagement</li></ul><p>observation은 추천 모델이 관찰하는 정보입니다. 여기서는 단순함을 위해 유저 feature는 고려하지 않습니다. 그러나 유저 feature는 실제로 매우 중요합니다. 예를 들어 성별을 고려 시, 상대적으로 여성이 남성보다 달콤한 음식을 선호하므로 이는 추천 시 중요한 feature가 될 수 있습니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="baselines">Baselines<a href="#baselines" class="hash-link" aria-label="Baselines에 대한 직접 링크" title="Baselines에 대한 직접 링크">​</a></h3><p>먼저, baseline으로 가장 달콤한 아이템만 추천하는 sweetest policy와 랜덤하게 추천하는 random policy를 사용하겠습니다. 아래는 두 베이스라인의 시간에 따른 reward 변화입니다.</p><p><img loading="lazy" src="/assets/images/sweetest-policy-rewards-9fc49136d81b6928377b8cae72f1dd95.png" width="680" height="524" class="img_ev3q"></p><p><img loading="lazy" src="/assets/images/random-policy-rewards-11dcf84163ae10b2a5af0be616e7fec3.png" width="680" height="524" class="img_ev3q"></p><ul><li>sweetest policy cumulative reward: 56.93+/-1.44</li><li>random policy cumulative reward: 98.41+/-24.32</li></ul><p>유저들은 대체적으로 달콤한 음식을 선호하지만, 시간이 지나면서 점점 달콤한 음식의 선호도가 내려감을 알 수 있습니다. cumulative reward는 모든 time step동안 획득한 reward의 총합입니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="rl-performance">RL Performance<a href="#rl-performance" class="hash-link" aria-label="RL Performance에 대한 직접 링크" title="RL Performance에 대한 직접 링크">​</a></h3><p>multi-armed bandit (MAB)는 기존에 추천 시스템에 많이 사용되던 방법입니다. short-term RL은 즉각적인 reward만 고려하는 강화학습 방법으로, MAB와 유사한 속성을 지니고 있습니다. 따라서 short-term RL을 통해 MAB의 문제점을 확인할 수 있습니다.</p><p>반대로 long-term RL은 future reward도 고려해 학습하는 방법입니다. 연속적인 의사 결정 문제에서 future reward에 대한 고려는 매우 중요합니다. 현재 선택한 action이 future reward에 영향을 미치기 때문입니다.</p><p><img loading="lazy" src="/assets/images/rl-performance-454a8682cb3246560c5639444400f599.png" width="760" height="604" class="img_ev3q"></p><ul><li>short-term RL: discount factor = 0</li><li>long-term RL: discount factor = 0.99</li></ul><p>short-term RL은 학습 결과 sweetest policy와 유사해집니다. 이는 너무 당연한게, 유저들은 대체적으로 달콤한 음식을 선호하기 때문에 즉각적인 reward가 높기 때문입니다. long-term RL은 future reward도 고려하기 때문에 결과적으로 cumulative reward가 가장 높습니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="references">References<a href="#references" class="hash-link" aria-label="References에 대한 직접 링크" title="References에 대한 직접 링크">​</a></h2><p>[1]<!-- --> Anyscale "<a href="https://github.com/anyscale/academy/tree/main/ray-rllib/acm_recsys_tutorial_2022" target="_blank" rel="noopener noreferrer">ACM RecSys 2022 Tutorial</a>"   (Github).<br>
<!-- -->[2]<!-- --> Ie, Eugene, et al. "<a href="https://arxiv.org/abs/1909.04847" target="_blank" rel="noopener noreferrer">Recsim: A configurable simulation platform for recommender systems.</a>  " arXiv preprint arXiv:1909.04847 (2019).<br>
<!-- -->[3]<!-- --> Lin, Yuanguo, et al. "<a href="https://ieeexplore.ieee.org/abstract/document/10144689?casa_token=bzipVczGG2wAAAAA:gkdWb-kk6v_bBlzY7Y3JLzwtsuWBrkw72iJE9Nm-r0uCB9ZDi_FCA-kwxbVTYlQjuOEi1BsW" target="_blank" rel="noopener noreferrer">A survey on reinforcement learning for recommender systems.</a>" IEEE Transactions on Neural Networks and Learning Systems (2023).<br>
<!-- -->[4]<!-- --> Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</p>]]></content>
        <author>
            <name>DevSlem</name>
            <uri>https://github.com/DevSlem</uri>
        </author>
        <category label="ai" term="ai"/>
        <category label="rl" term="rl"/>
        <category label="recsys" term="recsys"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Welcome]]></title>
        <id>https://compping.github.io/blog/welcome</id>
        <link href="https://compping.github.io/blog/welcome"/>
        <updated>2021-08-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Docusaurus blogging features are powered by the blog plugin.]]></summary>
        <content type="html"><![CDATA[<p><a href="https://docusaurus.io/docs/blog" target="_blank" rel="noopener noreferrer">Docusaurus blogging features</a> are powered by the <a href="https://docusaurus.io/docs/api/plugins/@docusaurus/plugin-content-blog" target="_blank" rel="noopener noreferrer">blog plugin</a>.</p><p>Simply add Markdown files (or folders) to the <code>blog</code> directory.</p><p>Regular blog authors can be added to <code>authors.yml</code>.</p><p>The blog post date can be extracted from filenames, such as:</p><ul><li><code>2019-05-30-welcome.md</code></li><li><code>2019-05-30-welcome/index.md</code></li></ul><p>A blog post folder can be convenient to co-locate blog post images:</p><p><img loading="lazy" alt="Docusaurus Plushie" src="/assets/images/docusaurus-plushie-banner-a60f7593abca1e3eef26a9afa244e4fb.jpeg" width="1500" height="500" class="img_ev3q"></p><p>The blog supports tags as well!</p><p><strong>And if you don't want a blog</strong>: just delete this directory, and use <code>blog: false</code> in your Docusaurus config.</p>]]></content>
        <author>
            <name>Sébastien Lorber</name>
            <uri>https://sebastienlorber.com</uri>
        </author>
        <author>
            <name>Yangshun Tay</name>
            <uri>https://github.com/yangshun</uri>
        </author>
        <category label="facebook" term="facebook"/>
        <category label="hello" term="hello"/>
        <category label="docusaurus" term="docusaurus"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[MDX Blog Post]]></title>
        <id>https://compping.github.io/blog/mdx-blog-post</id>
        <link href="https://compping.github.io/blog/mdx-blog-post"/>
        <updated>2021-08-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Blog posts support Docusaurus Markdown features, such as MDX.]]></summary>
        <content type="html"><![CDATA[<p>Blog posts support <a href="https://docusaurus.io/docs/markdown-features" target="_blank" rel="noopener noreferrer">Docusaurus Markdown features</a>, such as <a href="https://mdxjs.com/" target="_blank" rel="noopener noreferrer">MDX</a>.</p><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>팁</div><div class="admonitionContent_S0QG"><p>Use the power of React to create interactive blog posts.</p><div class="language-js codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-js codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">button onClick</span><span class="token operator" style="color:#393A34">=</span><span class="token punctuation" style="color:#393A34">{</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token arrow operator" style="color:#393A34">=&gt;</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">alert</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">'button clicked!'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">}</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token maybe-class-name">Click</span><span class="token plain"> me</span><span class="token operator" style="color:#393A34">!</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">button</span><span class="token operator" style="color:#393A34">&gt;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="클립보드에 코드 복사" title="복사" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><button>Click me!</button></div></div>]]></content>
        <author>
            <name>Sébastien Lorber</name>
            <uri>https://sebastienlorber.com</uri>
        </author>
        <category label="docusaurus" term="docusaurus"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long Blog Post]]></title>
        <id>https://compping.github.io/blog/long-blog-post</id>
        <link href="https://compping.github.io/blog/long-blog-post"/>
        <updated>2019-05-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This is the summary of a very long blog post,]]></summary>
        <content type="html"><![CDATA[<p>This is the summary of a very long blog post,</p><p>Use a <code>&lt;!--</code> <code>truncate</code> <code>--&gt;</code> comment to limit blog post size in the list view.</p><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet</p><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet</p><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet</p><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet</p><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet</p><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet</p><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet</p><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet</p><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet</p><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet</p><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet</p><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet</p><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet</p><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet</p><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet</p><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet</p>]]></content>
        <author>
            <name>Endilie Yacop Sucipto</name>
            <uri>https://github.com/endiliey</uri>
        </author>
        <category label="hello" term="hello"/>
        <category label="docusaurus" term="docusaurus"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[First Blog Post]]></title>
        <id>https://compping.github.io/blog/first-blog-post</id>
        <link href="https://compping.github.io/blog/first-blog-post"/>
        <updated>2019-05-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet]]></summary>
        <content type="html"><![CDATA[<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet</p>]]></content>
        <author>
            <name>Gao Wei</name>
            <uri>https://github.com/wgao19</uri>
        </author>
        <category label="hola" term="hola"/>
        <category label="docusaurus" term="docusaurus"/>
    </entry>
</feed>