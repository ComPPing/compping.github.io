<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>ppingpong Blog</title>
        <link>https://compping.github.io/blog</link>
        <description>ppingpong Blog</description>
        <lastBuildDate>Sun, 24 Sep 2023 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>ko</language>
        <item>
            <title><![CDATA[Conversational Recommender System Overall Processes]]></title>
            <link>https://compping.github.io/blog/conversational-recsys-overall-processes</link>
            <guid>https://compping.github.io/blog/conversational-recsys-overall-processes</guid>
            <pubDate>Sun, 24 Sep 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[이 포스트에서는 대화형 추천 시스템 (conversational recommender system)을 구축하기 위한 전반적인 과정과, 유저의 utterance와 같은 텍스트 데이터를 어떻게 처리할 것인가에 대해 간략히 다룹니다.]]></description>
            <content:encoded><![CDATA[<p>이 포스트에서는 대화형 추천 시스템 (conversational recommender system)을 구축하기 위한 전반적인 과정과, 유저의 utterance와 같은 텍스트 데이터를 어떻게 처리할 것인가에 대해 간략히 다룹니다.</p><p>유저의 utterance로부터 대화형 추천 시스템 구축은 자연어 처리 (natural language process), 추천 알고리즘, 유저 context에 대한 조합을 요구하는 복잡한 태스크입니다. 이를 step-by-step으로 알아봅시다:</p><ol><li><strong>Data Collection</strong>:<ul><li>이름, 위치, 타입 (e.g., bar, cafe, fine dining), 분위기 (e.g., quiet, lively), 음식 종류, 평점, 리뷰 등의 디테일한 정보를 포함하는 음식점 데이터셋을 모으는 걸로 시작합니다.</li><li>가능한, 유저의 선호도와 행동 데이터 (e.g., 이전에 방문한 음식점, 평점)를 모을 수 있으면 좋습니다.</li></ul></li><li><strong>Text Preprocessing</strong>:<ul><li>유저의 utterance을 처리하기 위해 NLP 기법을 사용합니다:<ul><li>Tokenization</li><li>Lowercasing</li><li>Removing stop words</li><li>Lemmatization or stemming</li></ul></li></ul></li><li><strong>Intent and Entity Recognition</strong>:<ul><li>utterance로부터 유저의 의도 (e.g., "find", "recommend", "search")를 추출합니다. 이때, intent recognition 모델을 사용합니다.</li><li>entity나 keyword (e.g., "quiet", "bar", "seafood")를 entity recognition이나 keyward extraction 기법을 사용해 추출합니다.</li></ul></li><li><strong>Contextual Understanding</strong>:<ul><li>시스템이 stateful하다면, 유저 context를 이해하기 위해 과거 상호작용들을 기억합니다. 이는 추천을 더 잘 정제할 수 있습니다. 예를 들어, 유저가 이전에 비건이라고 언급했다면, 고기집은 적절치 않은 추천입니다.</li><li>지역기반 서비스를 구축합니다. 유저가 특별히 다른 장소를 언급하지 않았다면, 유저 근처의 음식점 추천합니다.  </li></ul></li><li><strong>Candidate Generation</strong>:<ul><li>collaborative filtering이나 content-based filtering과 같은 전통적인 추천 알고리즘을 사용해 candidate list를 생성합니다.</li><li>추출된 entity를 바탕으로 이 리스트를 필터링합니다. 예를 들어, "quiet"와 "bar"가 entity라면, "quiet"한 attribute를 가진 bar만을 선택합니다.</li><li>유저의 행동 데이터가 있다면, 추천 리스트를 re-rank해 우선순위를 조정합니다. 예를 들어, 유저가 이탈리안 음식점을 자주 방문했다면, 이러한 음식점을 높은 우선순위에 둘 수 있습니다.</li></ul></li><li><strong>Hybrid Approach</strong>:<ul><li>추천 퀄리티를 높이기 위해 여러개의 추천 알고리즘을 결합할 수도 있습니다. 예를 들어, collaborative와 content-filtering을 결합하거나 딥러닝 기반 모델을 결합할 수 있습니다.</li></ul></li><li><strong>Feedback Loop</strong>:<ul><li>유저가 추천된 아이템들에 피드백을 제공하도록 합니다. explicit (e.g., ratings, likes)하거나 implicit (e.g., click-through rates, time spent)한 피드백들이 있습니다.</li><li>이러한 피드백을 사용해 지속적으로 추천 프로세스를 개선할 수 있습니다.</li></ul></li><li><strong>Post-Processing</strong>:<ul><li>유저에게 적절한 수의 추천 아이템을 제공합니다 (e.g., top 5 or top 10).</li><li>관련도, rating, 인기도와 같은 어떠한 metric을 사용해 추천된 아이템을 정렬합니다.</li></ul></li><li><strong>UI/UX Considerations</strong>:<ul><li>대화형 방식을 통해 context와 함께 추천 아이템을 제공합니다. 예를 들어, "근처 조용한 술집을 찾았어요:".</li><li>유저가 추천 시스템의 query를 조정하거나 추가 질문을 할 수 있게 합니다. 예를 들어, "이 중 실외석이 있는 곳이 있니?".</li></ul></li><li><strong>Evaluation and Continuous Learning</strong>:<ul><li>주기적으로 추천 시스템의 performance를 평가합니다. A/B testing과 같은 방법이 있습니다.</li><li>데이터와 피드백을 더 많이 수집하고, 추천 모델을 개선합니다.</li></ul></li></ol><p>이 중 텍스트 데이터를 처리하는 방법에 대해 집중적으로 다뤄보겠습니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-extract-features-from-text-data">How to Extract Features from Text Data<a href="#how-to-extract-features-from-text-data" class="hash-link" aria-label="How to Extract Features from Text Data에 대한 직접 링크" title="How to Extract Features from Text Data에 대한 직접 링크">​</a></h2><p>유저의 utterance와 같은 텍스트 데이터로부터 추천 시스템에 사용할 feature를 추출할 때 크게 2가지 방법으로 처리할 수 있습니다: named entity recognition, keyword extraction. 두 방법은 비슷하면서도 다릅니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="named-entity-recognition">Named Entity Recognition<a href="#named-entity-recognition" class="hash-link" aria-label="Named Entity Recognition에 대한 직접 링크" title="Named Entity Recognition에 대한 직접 링크">​</a></h3><p>named entity recognition (NER)은 일종의 tagging 작업입니다. 어떤 텍스트로부터 <strong>entity를 사전에 정의된 카테고리로 분류</strong>하는 정보 추출 작업입니다. 카테고리 예시로 음식 타입, 분위기, 위치 등이 있습니다. NER 모델은 labeled data로부터 학습됩니다. 각 문장 내 단어는 상응하는 entity 타입으로 태그되어있습니다. entity는 유스케이스에 따라 다르게 정의됩니다. 아래 그림에서 entity로 'Name', 'Date', 'Designation', 'Subject"가 사전에 정의되어있는걸 확인할 수 있습니다.</p><p><img loading="lazy" src="/assets/images/named-entity-recognition-4bb3151e0e888cfe931292003690ca58.png" width="1920" height="650" class="img_ev3q"></p><p><strong>Usage in candidate generation:</strong></p><ul><li>음식점 추천 시스템 맥락에서, NER은 유저 uterrance로부터 특정 entity를 식별합니다. 예를 들어, "시카고 내 이탈리안 음식점을 원해"라는 문장에서, "이탈리안"은 '음식 타입', "시카고"는 '위치'로 인식됩니다.</li><li>candidate generation은 식별된 entity에 대응하는 음식점을 찾습니다.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="keyword-extraction">Keyword Extraction<a href="#keyword-extraction" class="hash-link" aria-label="Keyword Extraction에 대한 직접 링크" title="Keyword Extraction에 대한 직접 링크">​</a></h3><p>keyward extraction은 거대한 텍스트 조각으로부터 관련된 용어를 식별 및 추출하는 작업입니다. 이는 entity가 사전에 정의되어있는가에 관계 없이 수행되며, 일종의 <strong>요약</strong>과 유사합니다.</p><p><strong>Usage in candidate generation:</strong></p><ul><li>"quiet", "rooftop", "seafood"와 같은 keyword를 유저의 utterance로부터 추출합니다.</li><li>이러한 keyword들은 음식점의 attribute, 태그, 리뷰 등과 매칭됩니다.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="our-case">Our Case<a href="#our-case" class="hash-link" aria-label="Our Case에 대한 직접 링크" title="Our Case에 대한 직접 링크">​</a></h3><p>추천 시스템 구축에는 NER이 조금 더 적합하다고 판단됩니다. 그러나, NER은 labeled data로부터의 학습이 필요하기 때문에, 데이터셋을 별도로 구축하고 학습까지 해야하는 문제가 있습니다.</p><p>GPT는 자연어에 대해 충분히 이해하고 추론할 수 있는 능력을 가지고 있습니다. GPT를 활용한다면 NER 작업을 쉽게 할 수 있을 거라 판단됩니다:</p><ol><li>음식점에 대한 entity를 사전에 잘 정의.</li><li>GPT에게 entity를 추출해달라고 프롬프팅.</li><li>유저 utterance 또는 아이템 description 입력.</li><li>추출된 entity를 추천 모델에 입력.</li></ol><p>실제, 간이 테스트 결과 GPT-3.5, GPT-4 모두 훌륭한 결과를 내놓았습니다.</p>]]></content:encoded>
            <category>recsys</category>
            <category>nlp</category>
            <category>ai</category>
        </item>
        <item>
            <title><![CDATA[Recommdenr System RL Introduction]]></title>
            <link>https://compping.github.io/blog/recsys-rl-intro</link>
            <guid>https://compping.github.io/blog/recsys-rl-intro</guid>
            <pubDate>Thu, 14 Sep 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[추천 시스템에 대해 더 자세히 이해하고 싶다면 아래 튜토리얼 코드를 참고하기 바랍니다.]]></description>
            <content:encoded><![CDATA[<p>추천 시스템에 대해 더 자세히 이해하고 싶다면 아래 튜토리얼 코드를 참고하기 바랍니다.</p><blockquote><p>Tutorial Code: <a href="https://github.com/DevSlem/recommender-system-rl-tutorial" target="_blank" rel="noopener noreferrer">DevSlem/recommender-system-rl-tutorial (Github)</a></p></blockquote><p><strong>추천 시스템</strong> (recommender system)은 유저의 선호도 (preference)에 맞는 아이템을 제공하는 시스템입니다. 이는 유저-아이템 상호작용 히스토리를 고려해 이루어지는데, 추천 시스템이 유저에게 아이템을 제공하면 유저는 이에 대해 <strong>피드백</strong> (스킵, 클릭, 구매 등)을 제공합니다. 유튜브, 넷플릭스 등 수 많은 어플리케이션에서 이러한 추천 시스템을 도입하고 있습니다.</p><p><img loading="lazy" src="/assets/images/recsys-d473d6e4f27bdd6eda6df5bef2d65719.png" width="668" height="374" class="img_ev3q"></p><p>추천 시스템은 머신 러닝 (machine learning)을 통해 구축할 수 있습니다. 지도학습 (supervised learning)과 같은 기존 방법들은 대체적으로 <strong>유저와 추천 모델 사이의 상호작용을 무시</strong>해 불만족스러운 결과를 내놓습니다. 일반적으로, 추천 시스템은 인터렉티브한 프로세스로 <strong>연속적인 의사 결정 문제</strong> (sequential decision making problem)입니다. 따라서 <strong>강화학습</strong> (reinforcement learning)을 사용하여 최적화할 수 있습니다. 아래 그림은 지도학습과 강화학습 기반 방법 사이의 성능 비교 테이블입니다.</p><p><img loading="lazy" src="/assets/images/recsys-performance-table-a04cf8567c3d7ca13e8da3c6403217af.png" width="1572" height="392" class="img_ev3q"></p><p>이 포스트에서는 강화학습으로 추천 시스템을 구축하는 것에 대한 간단한 소개를 하려고 합니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="recommdenr-system-process">Recommdenr System Process<a href="#recommdenr-system-process" class="hash-link" aria-label="Recommdenr System Process에 대한 직접 링크" title="Recommdenr System Process에 대한 직접 링크">​</a></h2><p>추천 시스템은 크게 두 과정으로 나뉩니다.</p><ol><li>candidate generation</li><li>ranking and recommendation</li></ol><p><img loading="lazy" src="/assets/images/recsys-process-3cdbcb35c9416067a3291e9592051b64.png" width="844" height="540" class="img_ev3q"></p><p>candidate generation은 수많은 아이템 중 일부분을 추출하는 과정입니다. 너무 많은 아이템을 모델에 입력하는 것은 비효율적이기 때문에 사전에 걸러내는 작업입니다. 이 때 후보 아이템 set을 <strong>document</strong>라고 부릅니다. ranking and recommendation은 document 아이템 중에서 실제 유저에게 추천할 아이템을 선택하는 과정입니다. 여기에 머신 러닝과 같은 기법이 사용됩니다. document로부터 선택된 아이템 set을 <strong>slate</strong>라고 부릅니다. </p><p><a href="https://github.com/google-research/recsim" target="_blank" rel="noopener noreferrer">Google RecSim</a>은 유저와의 연속적인 상호작용을 지원하는 추천 시스템에 대한 시뮬레이션 environment로, youtube 추천 알고리즘을 위해 개발되었습니다. 아래는 RecSim 아키텍쳐를 나타내는 그림으로 지금까지 설명한 내용을 한번에 보여주고 있습니다.</p><p><img loading="lazy" src="/assets/images/recsim-5dfba8e67865e1ed9962ac6af1123598.png" width="1032" height="686" class="img_ev3q"></p><p>자, 이제 toy 문제를 보고 왜 강화학습이 유용한지 알아봅시다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="problem-recommend-items-based-on-sweetness">Problem: Recommend Items based on Sweetness<a href="#problem-recommend-items-based-on-sweetness" class="hash-link" aria-label="Problem: Recommend Items based on Sweetness에 대한 직접 링크" title="Problem: Recommend Items based on Sweetness에 대한 직접 링크">​</a></h2><p>초콜릿과 케일 (채소)이 여러 개 있습니다. 우리는 초콜릿과 케일 중 어떤 것을 추천해야 유저가 만족할까를 고민하고 있습니다. 초콜릿은 단 맛이고, 케일은 쓴 맛이기 때문에 초콜릿과 케일을 달콤함 (sweetness)로 나타낼 수 있습니다. 여기서는 단순함을 위해 달콤함만 고려합시다. 유저들은 대체적으로 쓴 음식보다는 달콤한 음식을 선호할 것입니다. 우리가 생각해볼 수 있는 방법은 달콤한 음식만 추천하는 것입니다. 그러나 달콤한 음식만 추천하다보면 유저들은 점점 만족스러워하지 않을 것입니다. 왜냐하면 자신의 건강 역시 생각하기 떄문이죠. 따라서 유저들은 달콤한 음식보다는 건강에 좋은 달콤하지 않은 음식을 점점 더 선호하게 될 가능성이 있습니다. 그러나 대체적으로 달콤한 음식을 선호하는 유저들이 지속적으로 쓴 음식만 추천 받는다면 역시 불만족스럽겠죠. 이러한 요소들을 종합하면 우리의 가설은 다음과 같습니다: <strong>유저들은 대체적으로 달콤한 음식을 선호하지만, 시간이 지나면서 점점 달콤한 음식의 선호도가 내려가기 때문에 중간 중간 달콥하지 않은 음식도 추천 받길 원한다.</strong></p><p><img loading="lazy" src="/assets/images/user-choice-model-0a4b9ee6900e861beb837f4e049db2ab.png" width="1300" height="686" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="reinforcement-learning">Reinforcement Learning<a href="#reinforcement-learning" class="hash-link" aria-label="Reinforcement Learning에 대한 직접 링크" title="Reinforcement Learning에 대한 직접 링크">​</a></h2><p>위 문제를 강화학습으로 학습하기 위해 추천 시스템의 요소들을 잘 정의해야합니다. 추천 시스템의 궁극적인 목적은 유저의 engagement를 maximize하는 것입니다.</p><p>Objective: Maximize user's engagement.</p><p>여기서 engagment란 단어가 다소 모호할 수 있습니다. engagement란 추천된 아이템에 대한 상호작용이나 유저의 행동으로, <strong>유저의 흥미나 관심을 얼마나 효과적으로 끌고 있는지를 나타내는 측정값</strong>입니다. 예를 들면, 추천된 동영상을 시청한 시간 정도 입니다.</p><p>이제 다음을 정의해봅시다:</p><ul><li>Observation: sweetness of 20 items</li><li>Action: recommends 1 item</li><li>Reward: represents the engagement</li></ul><p>observation은 추천 모델이 관찰하는 정보입니다. 여기서는 단순함을 위해 유저 feature는 고려하지 않습니다. 그러나 유저 feature는 실제로 매우 중요합니다. 예를 들어 성별을 고려 시, 상대적으로 여성이 남성보다 달콤한 음식을 선호하므로 이는 추천 시 중요한 feature가 될 수 있습니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="baselines">Baselines<a href="#baselines" class="hash-link" aria-label="Baselines에 대한 직접 링크" title="Baselines에 대한 직접 링크">​</a></h3><p>먼저, baseline으로 가장 달콤한 아이템만 추천하는 sweetest policy와 랜덤하게 추천하는 random policy를 사용하겠습니다. 아래는 두 베이스라인의 시간에 따른 reward 변화입니다.</p><p><img loading="lazy" src="/assets/images/sweetest-policy-rewards-9fc49136d81b6928377b8cae72f1dd95.png" width="680" height="524" class="img_ev3q"></p><p><img loading="lazy" src="/assets/images/random-policy-rewards-11dcf84163ae10b2a5af0be616e7fec3.png" width="680" height="524" class="img_ev3q"></p><ul><li>sweetest policy cumulative reward: 56.93+/-1.44</li><li>random policy cumulative reward: 98.41+/-24.32</li></ul><p>유저들은 대체적으로 달콤한 음식을 선호하지만, 시간이 지나면서 점점 달콤한 음식의 선호도가 내려감을 알 수 있습니다. cumulative reward는 모든 time step동안 획득한 reward의 총합입니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="rl-performance">RL Performance<a href="#rl-performance" class="hash-link" aria-label="RL Performance에 대한 직접 링크" title="RL Performance에 대한 직접 링크">​</a></h3><p>multi-armed bandit (MAB)는 기존에 추천 시스템에 많이 사용되던 방법입니다. short-term RL은 즉각적인 reward만 고려하는 강화학습 방법으로, MAB와 유사한 속성을 지니고 있습니다. 따라서 short-term RL을 통해 MAB의 문제점을 확인할 수 있습니다.</p><p>반대로 long-term RL은 future reward도 고려해 학습하는 방법입니다. 연속적인 의사 결정 문제에서 future reward에 대한 고려는 매우 중요합니다. 현재 선택한 action이 future reward에 영향을 미치기 때문입니다.</p><p><img loading="lazy" src="/assets/images/rl-performance-454a8682cb3246560c5639444400f599.png" width="760" height="604" class="img_ev3q"></p><ul><li>short-term RL: discount factor = 0</li><li>long-term RL: discount factor = 0.99</li></ul><p>short-term RL은 학습 결과 sweetest policy와 유사해집니다. 이는 너무 당연한게, 유저들은 대체적으로 달콤한 음식을 선호하기 때문에 즉각적인 reward가 높기 때문입니다. long-term RL은 future reward도 고려하기 때문에 결과적으로 cumulative reward가 가장 높습니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="references">References<a href="#references" class="hash-link" aria-label="References에 대한 직접 링크" title="References에 대한 직접 링크">​</a></h2><p>[1]<!-- --> Anyscale "<a href="https://github.com/anyscale/academy/tree/main/ray-rllib/acm_recsys_tutorial_2022" target="_blank" rel="noopener noreferrer">ACM RecSys 2022 Tutorial</a>"   (Github).<br>
<!-- -->[2]<!-- --> Ie, Eugene, et al. "<a href="https://arxiv.org/abs/1909.04847" target="_blank" rel="noopener noreferrer">Recsim: A configurable simulation platform for recommender systems.</a>  " arXiv preprint arXiv:1909.04847 (2019).<br>
<!-- -->[3]<!-- --> Lin, Yuanguo, et al. "<a href="https://ieeexplore.ieee.org/abstract/document/10144689?casa_token=bzipVczGG2wAAAAA:gkdWb-kk6v_bBlzY7Y3JLzwtsuWBrkw72iJE9Nm-r0uCB9ZDi_FCA-kwxbVTYlQjuOEi1BsW" target="_blank" rel="noopener noreferrer">A survey on reinforcement learning for recommender systems.</a>" IEEE Transactions on Neural Networks and Learning Systems (2023).<br>
<!-- -->[4]<!-- --> Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</p>]]></content:encoded>
            <category>ai</category>
            <category>rl</category>
            <category>recsys</category>
        </item>
    </channel>
</rss>