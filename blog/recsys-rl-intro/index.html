<!doctype html>
<html lang="ko" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Recommdenr System RL Introduction | ppingpong</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://compping.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://compping.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://compping.github.io/blog/recsys-rl-intro"><meta data-rh="true" name="docusaurus_locale" content="ko"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="ko"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Recommdenr System RL Introduction | ppingpong"><meta data-rh="true" name="description" content="추천 시스템에 대해 더 자세히 이해하고 싶다면 아래 튜토리얼 코드를 참고하기 바랍니다."><meta data-rh="true" property="og:description" content="추천 시스템에 대해 더 자세히 이해하고 싶다면 아래 튜토리얼 코드를 참고하기 바랍니다."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2023-09-14T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/DevSlem"><meta data-rh="true" property="article:tag" content="ai,rl,recsys"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://compping.github.io/blog/recsys-rl-intro"><link data-rh="true" rel="alternate" href="https://compping.github.io/blog/recsys-rl-intro" hreflang="ko"><link data-rh="true" rel="alternate" href="https://compping.github.io/blog/recsys-rl-intro" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="ppingpong RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="ppingpong Atom Feed"><link rel="stylesheet" href="/assets/css/styles.810509e8.css">
<link rel="preload" href="/assets/js/runtime~main.f2c9c94d.js" as="script">
<link rel="preload" href="/assets/js/main.246bbaa5.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="본문으로 건너뛰기"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">본문으로 건너뛰기</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.png" alt="logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.png" alt="logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">ppingpong</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">블로그</a><a class="navbar__item navbar__link" href="/docs/intro">회의록</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/ComPPing" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="어두운 모드와 밝은 모드 전환하기 (현재 밝은 모드)" aria-label="어두운 모드와 밝은 모드 전환하기 (현재 밝은 모드)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="최근 블로그 문서 둘러보기"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2023/11/04/oauth2 흐름(카카오,구글)">Oauth2 흐름(카카오, 구글)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/conversational-recsys-dialogue-management">Conversational Recommender System - Dialogue Management</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/nextjs-init">Next13 + cloudflare + PWA를 이용한 초기세팅</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/conversational-recsys-overall-processes">Conversational Recommender System Overall Processes</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/blog/recsys-rl-intro">Recommdenr System RL Introduction</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="title_f1Hy" itemprop="headline">Recommdenr System RL Introduction</h1><div class="container_mt6G margin-vert--md"><time datetime="2023-09-14T00:00:00.000Z" itemprop="datePublished">2023년 9월 14일</time> · <!-- -->약 10분</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/DevSlem" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/u/69196755?v=4" alt="박진영"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/DevSlem" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">박진영</span></a></div><small class="avatar__subtitle" itemprop="description">AI Research Engineer</small></div></div></div></div></header><div id="__blog-post-container" class="markdown" itemprop="articleBody"><p>추천 시스템에 대해 더 자세히 이해하고 싶다면 아래 튜토리얼 코드를 참고하기 바랍니다.</p><blockquote><p>Tutorial Code: <a href="https://github.com/DevSlem/recommender-system-rl-tutorial" target="_blank" rel="noopener noreferrer">DevSlem/recommender-system-rl-tutorial (Github)</a></p></blockquote><p><strong>추천 시스템</strong> (recommender system)은 유저의 선호도 (preference)에 맞는 아이템을 제공하는 시스템입니다. 이는 유저-아이템 상호작용 히스토리를 고려해 이루어지는데, 추천 시스템이 유저에게 아이템을 제공하면 유저는 이에 대해 <strong>피드백</strong> (스킵, 클릭, 구매 등)을 제공합니다. 유튜브, 넷플릭스 등 수 많은 어플리케이션에서 이러한 추천 시스템을 도입하고 있습니다.</p><p><img loading="lazy" src="/assets/images/recsys-d473d6e4f27bdd6eda6df5bef2d65719.png" width="668" height="374" class="img_ev3q"></p><p>추천 시스템은 머신 러닝 (machine learning)을 통해 구축할 수 있습니다. 지도학습 (supervised learning)과 같은 기존 방법들은 대체적으로 <strong>유저와 추천 모델 사이의 상호작용을 무시</strong>해 불만족스러운 결과를 내놓습니다. 일반적으로, 추천 시스템은 인터렉티브한 프로세스로 <strong>연속적인 의사 결정 문제</strong> (sequential decision making problem)입니다. 따라서 <strong>강화학습</strong> (reinforcement learning)을 사용하여 최적화할 수 있습니다. 아래 그림은 지도학습과 강화학습 기반 방법 사이의 성능 비교 테이블입니다.</p><p><img loading="lazy" src="/assets/images/recsys-performance-table-a04cf8567c3d7ca13e8da3c6403217af.png" width="1572" height="392" class="img_ev3q"></p><p>이 포스트에서는 강화학습으로 추천 시스템을 구축하는 것에 대한 간단한 소개를 하려고 합니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="recommdenr-system-process">Recommdenr System Process<a href="#recommdenr-system-process" class="hash-link" aria-label="Recommdenr System Process에 대한 직접 링크" title="Recommdenr System Process에 대한 직접 링크">​</a></h2><p>추천 시스템은 크게 두 과정으로 나뉩니다.</p><ol><li>candidate generation</li><li>ranking and recommendation</li></ol><p><img loading="lazy" src="/assets/images/recsys-process-3cdbcb35c9416067a3291e9592051b64.png" width="844" height="540" class="img_ev3q"></p><p>candidate generation은 수많은 아이템 중 일부분을 추출하는 과정입니다. 너무 많은 아이템을 모델에 입력하는 것은 비효율적이기 때문에 사전에 걸러내는 작업입니다. 이 때 후보 아이템 set을 <strong>document</strong>라고 부릅니다. ranking and recommendation은 document 아이템 중에서 실제 유저에게 추천할 아이템을 선택하는 과정입니다. 여기에 머신 러닝과 같은 기법이 사용됩니다. document로부터 선택된 아이템 set을 <strong>slate</strong>라고 부릅니다. </p><p><a href="https://github.com/google-research/recsim" target="_blank" rel="noopener noreferrer">Google RecSim</a>은 유저와의 연속적인 상호작용을 지원하는 추천 시스템에 대한 시뮬레이션 environment로, youtube 추천 알고리즘을 위해 개발되었습니다. 아래는 RecSim 아키텍쳐를 나타내는 그림으로 지금까지 설명한 내용을 한번에 보여주고 있습니다.</p><p><img loading="lazy" src="/assets/images/recsim-5dfba8e67865e1ed9962ac6af1123598.png" width="1032" height="686" class="img_ev3q"></p><p>자, 이제 toy 문제를 보고 왜 강화학습이 유용한지 알아봅시다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="problem-recommend-items-based-on-sweetness">Problem: Recommend Items based on Sweetness<a href="#problem-recommend-items-based-on-sweetness" class="hash-link" aria-label="Problem: Recommend Items based on Sweetness에 대한 직접 링크" title="Problem: Recommend Items based on Sweetness에 대한 직접 링크">​</a></h2><p>초콜릿과 케일 (채소)이 여러 개 있습니다. 우리는 초콜릿과 케일 중 어떤 것을 추천해야 유저가 만족할까를 고민하고 있습니다. 초콜릿은 단 맛이고, 케일은 쓴 맛이기 때문에 초콜릿과 케일을 달콤함 (sweetness)로 나타낼 수 있습니다. 여기서는 단순함을 위해 달콤함만 고려합시다. 유저들은 대체적으로 쓴 음식보다는 달콤한 음식을 선호할 것입니다. 우리가 생각해볼 수 있는 방법은 달콤한 음식만 추천하는 것입니다. 그러나 달콤한 음식만 추천하다보면 유저들은 점점 만족스러워하지 않을 것입니다. 왜냐하면 자신의 건강 역시 생각하기 떄문이죠. 따라서 유저들은 달콤한 음식보다는 건강에 좋은 달콤하지 않은 음식을 점점 더 선호하게 될 가능성이 있습니다. 그러나 대체적으로 달콤한 음식을 선호하는 유저들이 지속적으로 쓴 음식만 추천 받는다면 역시 불만족스럽겠죠. 이러한 요소들을 종합하면 우리의 가설은 다음과 같습니다: <strong>유저들은 대체적으로 달콤한 음식을 선호하지만, 시간이 지나면서 점점 달콤한 음식의 선호도가 내려가기 때문에 중간 중간 달콥하지 않은 음식도 추천 받길 원한다.</strong></p><p><img loading="lazy" src="/assets/images/user-choice-model-0a4b9ee6900e861beb837f4e049db2ab.png" width="1300" height="686" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="reinforcement-learning">Reinforcement Learning<a href="#reinforcement-learning" class="hash-link" aria-label="Reinforcement Learning에 대한 직접 링크" title="Reinforcement Learning에 대한 직접 링크">​</a></h2><p>위 문제를 강화학습으로 학습하기 위해 추천 시스템의 요소들을 잘 정의해야합니다. 추천 시스템의 궁극적인 목적은 유저의 engagement를 maximize하는 것입니다.</p><p>Objective: Maximize user&#x27;s engagement.</p><p>여기서 engagment란 단어가 다소 모호할 수 있습니다. engagement란 추천된 아이템에 대한 상호작용이나 유저의 행동으로, <strong>유저의 흥미나 관심을 얼마나 효과적으로 끌고 있는지를 나타내는 측정값</strong>입니다. 예를 들면, 추천된 동영상을 시청한 시간 정도 입니다.</p><p>이제 다음을 정의해봅시다:</p><ul><li>Observation: sweetness of 20 items</li><li>Action: recommends 1 item</li><li>Reward: represents the engagement</li></ul><p>observation은 추천 모델이 관찰하는 정보입니다. 여기서는 단순함을 위해 유저 feature는 고려하지 않습니다. 그러나 유저 feature는 실제로 매우 중요합니다. 예를 들어 성별을 고려 시, 상대적으로 여성이 남성보다 달콤한 음식을 선호하므로 이는 추천 시 중요한 feature가 될 수 있습니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="baselines">Baselines<a href="#baselines" class="hash-link" aria-label="Baselines에 대한 직접 링크" title="Baselines에 대한 직접 링크">​</a></h3><p>먼저, baseline으로 가장 달콤한 아이템만 추천하는 sweetest policy와 랜덤하게 추천하는 random policy를 사용하겠습니다. 아래는 두 베이스라인의 시간에 따른 reward 변화입니다.</p><p><img loading="lazy" src="/assets/images/sweetest-policy-rewards-9fc49136d81b6928377b8cae72f1dd95.png" width="680" height="524" class="img_ev3q"></p><p><img loading="lazy" src="/assets/images/random-policy-rewards-11dcf84163ae10b2a5af0be616e7fec3.png" width="680" height="524" class="img_ev3q"></p><ul><li>sweetest policy cumulative reward: 56.93+/-1.44</li><li>random policy cumulative reward: 98.41+/-24.32</li></ul><p>유저들은 대체적으로 달콤한 음식을 선호하지만, 시간이 지나면서 점점 달콤한 음식의 선호도가 내려감을 알 수 있습니다. cumulative reward는 모든 time step동안 획득한 reward의 총합입니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="rl-performance">RL Performance<a href="#rl-performance" class="hash-link" aria-label="RL Performance에 대한 직접 링크" title="RL Performance에 대한 직접 링크">​</a></h3><p>multi-armed bandit (MAB)는 기존에 추천 시스템에 많이 사용되던 방법입니다. short-term RL은 즉각적인 reward만 고려하는 강화학습 방법으로, MAB와 유사한 속성을 지니고 있습니다. 따라서 short-term RL을 통해 MAB의 문제점을 확인할 수 있습니다.</p><p>반대로 long-term RL은 future reward도 고려해 학습하는 방법입니다. 연속적인 의사 결정 문제에서 future reward에 대한 고려는 매우 중요합니다. 현재 선택한 action이 future reward에 영향을 미치기 때문입니다.</p><p><img loading="lazy" src="/assets/images/rl-performance-454a8682cb3246560c5639444400f599.png" width="760" height="604" class="img_ev3q"></p><ul><li>short-term RL: discount factor = 0</li><li>long-term RL: discount factor = 0.99</li></ul><p>short-term RL은 학습 결과 sweetest policy와 유사해집니다. 이는 너무 당연한게, 유저들은 대체적으로 달콤한 음식을 선호하기 때문에 즉각적인 reward가 높기 때문입니다. long-term RL은 future reward도 고려하기 때문에 결과적으로 cumulative reward가 가장 높습니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="references">References<a href="#references" class="hash-link" aria-label="References에 대한 직접 링크" title="References에 대한 직접 링크">​</a></h2><p>[1]<!-- --> Anyscale &quot;<a href="https://github.com/anyscale/academy/tree/main/ray-rllib/acm_recsys_tutorial_2022" target="_blank" rel="noopener noreferrer">ACM RecSys 2022 Tutorial</a>&quot;   (Github).<br>
<!-- -->[2]<!-- --> Ie, Eugene, et al. &quot;<a href="https://arxiv.org/abs/1909.04847" target="_blank" rel="noopener noreferrer">Recsim: A configurable simulation platform for recommender systems.</a>  &quot; arXiv preprint arXiv:1909.04847 (2019).<br>
<!-- -->[3]<!-- --> Lin, Yuanguo, et al. &quot;<a href="https://ieeexplore.ieee.org/abstract/document/10144689?casa_token=bzipVczGG2wAAAAA:gkdWb-kk6v_bBlzY7Y3JLzwtsuWBrkw72iJE9Nm-r0uCB9ZDi_FCA-kwxbVTYlQjuOEi1BsW" target="_blank" rel="noopener noreferrer">A survey on reinforcement learning for recommender systems.</a>&quot; IEEE Transactions on Neural Networks and Learning Systems (2023).<br>
<!-- -->[4]<!-- --> Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</p></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>태그:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ai">ai</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/rl">rl</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/recsys">recsys</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-09-14-recsys-rl-intro.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>페이지 편집</a></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="블로그 게시물 탐색"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/conversational-recsys-overall-processes"><div class="pagination-nav__sublabel">이전 게시물</div><div class="pagination-nav__label">Conversational Recommender System Overall Processes</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#recommdenr-system-process" class="table-of-contents__link toc-highlight">Recommdenr System Process</a></li><li><a href="#problem-recommend-items-based-on-sweetness" class="table-of-contents__link toc-highlight">Problem: Recommend Items based on Sweetness</a></li><li><a href="#reinforcement-learning" class="table-of-contents__link toc-highlight">Reinforcement Learning</a><ul><li><a href="#baselines" class="table-of-contents__link toc-highlight">Baselines</a></li><li><a href="#rl-performance" class="table-of-contents__link toc-highlight">RL Performance</a></li></ul></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 ComPPing</div></div></div></footer></div>
<script src="/assets/js/runtime~main.f2c9c94d.js"></script>
<script src="/assets/js/main.246bbaa5.js"></script>
</body>
</html>